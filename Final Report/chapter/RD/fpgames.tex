\section{Literature Review: Functional Programming for Games}
\label{sec:fp_review}

\label{cf:code_organisation} % Reference from architecture section on OO vs FP code structure

% Literature review of existing work
% What this review is
% Less about games, more about Haskell as a real world language ==> more literature
% Discuss paper, critique, explain relevance

This section will discuss the suitability of the functional approach, the use
of Haskell in particular, in the real world. It will look at past projects and
research into the use of functional programming languages in industry in an
attempt to discover how functional programming helped or hindered development.
% More intro

John Hughes' paper ``Why functional programming matters'' aims to demonstrate how
``vitally important'' functional programming is to the real world by exploring and
demonstrating its advantages.\cite{hughes1989whyfp} Hughes argues that modularity
is the key to designing and implementing successful programs for three main reasons.
Firstly, small modules are much easier to code quickly because the requirements for
a small component are much easier to reason about, design, and implement. Second,
the more generic modules that are constructed can be reused. This leads to faster
development during subsequent projects. Thirdly, the independence of modules allows
them to developed and tested separately, helping to parallelise the work that needs
to be done and reducing the amount of time required for debugging. These advantages
of modular design combine to bring great improvements to productivity. These benefits
of modularisation are also espoused by Parnas who agrees that modular programming
shortens development time, improves comprehensibility of the resulting programs,
and increases flexibility --- it is possible to make large changes to one module
without affecting any of the others.\cite{parnas1972modular}

However, the ability of the programmer to modularise their code is reliant on the
ways in which they can glue the solutions of subproblems together. This glue must
often be provided by the programming language itself. Hughes argues that functional
programming provides two very important kinds of glue: higher order functions
and lazy evaluation. These two aspects of functional programming are very powerful
and allow greatly improved modularisation.

\functions(reduce)
General higher order functions, such as "map" and "reduce"\sidenote{The \scalenote{"reduce"} function is called \scalenote{"foldl"} in Haskell}, can be used as glue for
simpler, specialised functions to make more complex ones. Higher order functions
are great examples of code reuse as they can be used to create many other functions
with minimal effort. Hughes gives examples of operations over lists and trees, such
as summing up the elements of a list, whose implementation is greatly simplified
by the use of higher order functions. Lazy evaluation, on the other hand, allows
whole programs to be glued together. When composing two programs it might be
infeasible to store the entirety of the output of the first function in memory to
pass on to the second. Lazy evaluation is a solution to this problem. The output
function is only started when the input to the second function is required, and
only runs for long enough to provide the required amount of input. If the consuming
function terminates early then the producer can also quit. This even allows the
producer to create an infinite amount of output. This allows modularisation by
constructing a generator that outputs a large set of potential answers and a
separate selector that chooses the correct one.

Hughes finishes with an example from the field of artificial intelligence, a
field of computer science that is very relevant to game development. He shows
how the alpha-beta pruning algorithm can be constructed relatively simply using
modularisation through higher order functions and lazy evaluation. The algorithm
works by generating the entire set of possible game states that are reachable
from the current position. This list can then be lazily evaluated to find the
optimal move, but without actually constructing the entire, possibly infinite,
game tree. Higher order functions are used throughout to build up complex
functions from simpler ones. Hughes also shows that due to the modularisation
of the example it is much easier to understand and make modifications to the
program.

This paper is a great example of the power of that is available from the functional
approach. Giving real examples Hughes is able to make a strong case for the
effectiveness of modularisation through laziness and higher order functions.
The demonstration of a highly modular version of the alpha-beta pruning algorithm,
in particular, is of great interest due to its applicability to game development.
The conclusion that the functional approach leads to more general, reusable
modules is supported by John Backus' ACM Turing award lecture from 1977. Backus
gives the example of a program to calculate the inner product and finds that
``the functional version is nonrepetitive, \ldots is more hierarchically constructed,
is completely general, and creates `housekeeping' operations by composing high-level
housekeeping operators.''\cite{backus1978liberate}

In his 1987 paper ``No Silver Bullet'', Brooks identified four difficulties that
are inherent in the nature of software development: complexity, conformity,
changeability, and invisibility.\cite{brooks1987bullet} Brooks believes that
these four essential difficulties make it very unlikely that there will be a
``single development, in either technology or in management technique,
that by itself promises even one order-of-magnitude improvement in productivity,
in reliability, in simplicity.'' However, Moseley and Marks argue that complexity
is the only significant problem; that ``complexity is \emph{the} root cause of the
vast majority of problems with software today.''\cite{moseley2006tarpit} Other
problems can either be classified as complexity, or derive from unmanageable
complexity. They argue that simplicity is vital to successful software development,
and that functional programming can help to deliver this simplicity.

Moseley and Marks indentify several causes of complexity in real software systems.
The first cause is mutable state. Brooks also mentions the problem of state,
saying that from ``the difficulty of enumerating, much less understanding, all
the possible states of the program, \ldots comes the unreliability''.\cite{brooks1987bullet}
State hinders understanding of software through testing and reasoning about the code.
This is because testing a program in one state does not guarantee anything about
how the program will behave when in a different state. The vast number of different
possible states also makes it infeasible to understand them all. The functional
solution to the complexity of state is to discard state and side effects.
Programming with a pure functional language, such as Haskell, creates referentially
transparent programs. Referential transparency means that given the same set of
arguments a function will \emph{always} return the same result. Removing state
and side effects eases understanding of programs because they are easier to
reason about and test: ``avoiding side effects has serendipitous effects on testing.''\cite{smallbone2011}

However, Moseley and Marks suggest that ``the main weakness of functional programming
is the flip side of its main strength --- namely that problems arise when (as is often the
case) the system to be built must maintain state of some kind.'' Games are an
example of programs that must keep some kind of state, such as a score or the
positions of entities in the world. Is it possible to simulate the necessary state
in a functional language that has removed mutable state? One possibility would be
to add a new parameter and change the return type of functions to allow them to
accept and output state. In this way the state can be threaded through the entire
program. Moseley and Marks point out that this would recreate a pool of global
variables and, although referential transparency is maintained, the ease of understanding
is lost. The all important concept of modularity is raised again by Wadler who
notes that ``it is with regard to modularity that explicit data flow becomes both
a blessing and a curse.''\cite{wadler1995monads} He describes explicit data flow
as ``the ultimate in modularity'' since all data in and out is seen clearly and is
accessible. On the other hand, ``the essence of an algorithm can become buried under
the plumbing required to carry data from its point of creation to its point of use.''
The other approach, applicable to Haskell, is to use monads. Wadler explains that
monads can be used to ``mimic the effect of impure features such as exceptions,
state, and continuations.''\cite{wadler1992essence} The use of monads in functional
programming allows a developer to work with state without drowning under the huge
amount of explicit data flow required in the former approach. Although Moseley and Marks
are still concerned that ``despite their huge strengths monads have as yet been
insufficient to give rise to widespread adoption of functional techniques.''

The second cause of complexity identified by Moseley and Marks is complexity
from control. Control is the order in which things happen within a program.
In most programming languages the developer is concerned with control because
often the ordering is controlled by the order in which code appears in a program
and because this order is further modified by branching and looping instructions.
The problem with control is that it hinders informal reasoning about a program.
A reviewer must assume that the ordering of statements is significant until
proven otherwise. If a mistake is made in this process then subtle bugs can
be introduced into a program. Functional programming helps slightly with this
problem since the approach encourages more abstract control with functions
such as "map" instead of explicit loops. Also, due to the referentially transparent
nature of functional programming, the order of execution of function calls
is irrelevant.\cite{hughes1989whyfp}\cite{wadler1995monads}

The final major cause of complexity is code volume. Large, bloated programs
require much more effort to fully understand. Brooks believed that the complexity
of a software project increases nonlinearly with its size.\cite{brooks1987bullet}
For this reason it is ``vital to reduce the amount of code to an absolute
minimum''.\cite{moseley2006tarpit} The functional approach to programming
has been noted to produce much more concise programs. For example, Hughes
states that ``functional programs are an order of magnitude shorter'' than
their conventional counterparts.\cite{hughes1989whyfp} Moseley and Marks
also argue that by reducing the complexity caused by state and control it
is much less likely that complexity with grow with code volume in a nonlinear
fashion, citing Dijkstra\cite{dijkstra1972humble}:

\begin{quote}
It has been suggested that there is some kind of law of nature telling us that
the amount of intellectual effort needed grows with the square of program length.
But, thank goodness, no one has been able to prove this law. And this is because
it need not be true\ldots As a result I tend to the assumption --- up till now
not disproved by experience --- that by suitable application of our powers of
abstraction, the intellectual effort needed to conceive or to understand a program
need not grow more than proportional to program length.
\end{quote}
\noindent
It has been shown in the explorations of the previous two causes of complexity
that functional programming can help to reduce the complexity of state and
control. Therefore, the issue of code volume may be less of a cause for complexity
than in other programming paradigms.

Common misonceptions surround the use of functional languages for practical
software projects. Many seem to believe that functional programming restricts
a developer; that it is too hard to build graphical programs, work with input
and output, or perform other stateful computation, such as networking.
The author of the darcs version control system laments that a common reaction
from people hearing about darcs is to say that ``it is a shame that it is
written in Haskell''.\cite{roundy2005darcs} They believe that, because it is
written in Haskell, darcs will be inefficient, hogging memory and running slowly.
Roundy then goes on to discuss the problems and successes he encountered whilst
developing darcs in Haskell to show how Haskell can be used to build useful, real
world programs.

Roundy talks about testing with Haskell and the power of the QuickCheck library.
QuickCheck is a property based testing library that requires the developer to
create specifications for their code. QuickCheck will then automatically generate
test cases for these expected properties.\cite{claessen2000} Testing is extremely
important aspect of good software development. Therefore, any programming language
that is going to be used for real software projects requires good support for
testing. The availability of testing libraries for Haskell that have been used
successfully in existing projects is a good sign for the suitability of Haskell
for developing real world applications. The same day that Roundy started making
use of QuickCheck he was able to discover and fix a bug. However, he found that
it was sometimes hard to develop custom data generators which worked correctly.
Often it was found that test cases failed because of invalid patches being generated
instead of bugs in the darcs code itself.

Roundy goes on to talk about how essential the foreign function interface (FFI)
was for the development of darcs. The FFI is used to links Haskell programs to
other programs written in a different language, such as C. In darcs, for example,
the FFI was used to interface with \texttt{libcurl} for HTTP support. The necessity
for the FFI suggests that functional programming may not be suitable for all problems
and that complex, functional projects might have to `resort' to making use of
non-functional libraries. On the other hand, this paper was written in 2005, so
the number of Haskell libraries for common problems will have increased. So,
resorting to non-functional libraries is less likely to be required for common
problems.

Roundy also talks of the difficulty of optimisation in Haskell. He states that
increasing laziness at a high level often helps to improve memory usage, whilst
increasing strictness at lower levels usually makes functions faster. However,
the difficulty is in determining which approach to take to optimise a given
function and it is almost never obvious how a change will affect the laziness
of a function. Efficiency is an important requirement for real time games, so
difficulties with optimisation may have a negative impact on the quality of a
game. On the other hand, Roundy praises the utility of the profiling tools that
are available for Haskell. Using these tools it is much easier to pinpoint the
areas of code to focus optimisation efforts on.

Deciding how to optimise a function in Haskell is not the only difficultly. It
may require dropping into another language. Roundy states that for the lowest
level functions ``optimisation has consisted of rewriting a key function in C
or calling a C library function''. Again, this is not a good sign for the performance
of functional programming. However, in the eight years since this paper was
written, a large number of performance improvements have been made to Haskell
compilers. This means that functional programs written today are more likely
to perform well. It may also be the case that the optimisations that were made
to darcs could have been made in a different manner whilst still making use of
Haskell.

Roundy concludes that darcs has been a highly successful project written in
Haskell. His comments support the ideas of modularity proposed by Hughes stating
that ``Haskell itself allows the creation of clean internal interfaces in the
code''. These clearly separate modules allow contributors to focus on certain
areas instead of having to learn the entire code base and all of its iteractions.
And, although there have been efficiency problems in the past, they have mostly
been fixed.
